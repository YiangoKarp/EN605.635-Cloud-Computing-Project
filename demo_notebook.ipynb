{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes = pd.read_csv('data/Police_Department_Incident_Reports__2018_to_Present.csv', \n",
    "                      encoding = 'ISO-8859-1',\n",
    "                      usecols = ['Incident Datetime', 'Row ID', 'Report Type Description', 'Incident Description', 'Incident Category', 'Incident Subcategory', 'Police District', 'Latitude', 'Longitude'])\n",
    "new_names = {'Incident Datetime': 'datetime',\n",
    "             'Row ID': 'row_id',\n",
    "             'Report Type Description': 'report_type_description',\n",
    "             'Incident Description': 'incident_description',\n",
    "             'Incident Category': 'incident_category',\n",
    "             'Incident Subcategory': 'incident_subcategory',\n",
    "             'Police District': 'police_district',\n",
    "             'Latitude': 'latitude',\n",
    "             'Longitude': 'longitude'}\n",
    "\n",
    "crimes.rename(columns=new_names, inplace=True)\n",
    "crimes = crimes[(crimes['latitude'] != 0) & (crimes['longitude'] != 0)] # Remove bad geolocation data\n",
    "crimes.dropna(subset=['latitude', 'longitude'], inplace=True)\n",
    "def parse_date(date_str):\n",
    "    return datetime.strptime(date_str, '%Y/%m/%d %I:%M:%S %p').strftime('%Y-%m-%dT%H:%M:%S%z')\n",
    "crimes['datetime'] = crimes['datetime'].apply(parse_date)\n",
    "\n",
    "def combine_lat_long(df):\n",
    "    df['coordinate'] = df['latitude'].astype(str) + ',' + df['longitude'].astype(str)\n",
    "    df.drop(['latitude', 'longitude'], axis=1, inplace=True)\n",
    "    return df\n",
    "crimes = combine_lat_long(crimes)\n",
    "crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_documents = crimes.to_dict(orient=\"records\")\n",
    "crimes_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenSearch(\n",
    "    hosts = [{\"host\": \"localhost\", \"port\": 9200}],\n",
    "    http_auth = (\"admin\", \"admin\"),\n",
    "    use_ssl = True,\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False,\n",
    ")\n",
    "client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.indices.delete('crimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the index was already created\n",
    "if not client.indices.exists(index=\"crimes\"):\n",
    "    # Create the crimes index using a mapping\n",
    "    mapping = {\n",
    "            \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"datetime\": {\"type\": \"date\"},\n",
    "                \"row_id\": {\"type\": \"keyword\"},\n",
    "                \"report_type_description\": {\"type\": \"text\"},\n",
    "                \"incident_category\": {\"type\": \"keyword\"},\n",
    "                \"incident_subcategory\": {\"type\": \"keyword\"},\n",
    "                \"incident_description\": {\"type\": \"text\"},\n",
    "                \"police_district\": {\"type\": \"keyword\"},\n",
    "                \"coordinate\": {\"type\": \"geo_point\"},\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    client.indices.create(index=\"crimes\", body=mapping)\n",
    "    print(\"Created crimes index.\")\n",
    "else:\n",
    "    print(\"The crimes index already exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000 # Number of documents in a chunk.\n",
    "chunks = [crimes_documents[index:index + chunk_size] for index in range(0, len(crimes_documents), chunk_size)]\n",
    "\n",
    "# Process each chunk\n",
    "for chunk in tqdm(chunks, desc = 'Processing documents', unit = 'chunk'):\n",
    "    body_list = [] # Save all the OpenSearch commands for this chunk. \n",
    "\n",
    "    # Add each document to the bulk insert operation\n",
    "    for doc in chunk:\n",
    "        body_list.append({'index': {'_index': 'crimes', '_id': doc['row_id']}})\n",
    "        body_list.append(doc)\n",
    "    \n",
    "    response = client.bulk(body='\\n'.join([json.dumps(b) for b in body_list])+'\\n') # Commence bulk inserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
